{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Scripts Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project\n",
    "\n",
    "Imagine your job was to produce a TV script for the next episode of a long running TV show. There were so many episodes and it is hard for you to come up with something new. Good news is you can use deep learning to write the script for you. There are many scripts from previous episodes that a nerual network can learn from. And what's important, the audience likes what it is used to, so it sounds like a perfect solution.\n",
    "\n",
    "In this project I'm going to train a neural network using scripts from the 9 seasons of Seinfeld show. Based on recognized patterns, it will be able to generate a new text. And I will use it to create a one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "If you look at the data file, you will find out that it contains lines from scripts, appended episode by episode. I'm loading them all as a one huge string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "# Load in data\n",
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can access `text` letter by letter. Here I print first 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \\n\\njerry: (pointi'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that new lines are represented by `\\n`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `view_line_range` I print out the first ten lines of text. Empty lines also count as lines, so in result it printed out 5 quotes.\n",
    "\n",
    "You can also see some dataset stats. Rough estimate of unique words is overshot as it considers everything between spaces as a single word. So it will consider *back!* and *back* as two different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544240293684143\n",
      "\n",
      "The lines 0 to 10:\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
      "\n",
      "george: are you through? \n",
      "\n",
      "jerry: you do of course try on, when you buy? \n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Determine which lines range to print\n",
    "view_line_range = (0, 10)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "\n",
    "While working with text data, it is useful to code words into numbers. I am going to do it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup tables\n",
    "\n",
    "I will create two dictionaries, `vocab_to_int` that will map words to integers and `int_to_vocab` that will do the opposite. These lookup tables will make it easy to translate between words and their integer codings later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    '''Create lookup tables for vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        text(str): The text of tv scripts split into words\n",
    "    \n",
    "    Return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    '''\n",
    "    # Transform text into a large tuple of unique words\n",
    "    vocab = tuple(set(text))\n",
    "    \n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    vocab_to_int = { vocab : i for i, vocab in int_to_vocab.items() }\n",
    "\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "# Test\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize punctuation\n",
    "\n",
    "Later, I'm going to split `text` into words using whitespaces. However, punctuation can cause some problems like for example recognizning *bye* and *bye!* as two different words. You can solve it by replacing every punctuation mark with some word token, like \".\" with \"||period||\". You might ask, why to add separators \"|\"? It's so during analysis you can distinguish between real words and the tokens.\n",
    "\n",
    "Now I'm going to create a dictionary that can map punctuation marks into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    '''Generate a dict to turn punctuation into a token.\n",
    "    \n",
    "    Return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    '''\n",
    "    punctuation_to_token = {\n",
    "        '.' : '||period||',\n",
    "        ',' : '||comma||',\n",
    "        '\"' : '||quotation_mark||',\n",
    "        ';' : '||semicolon||',\n",
    "        '!' : '||exclamation_mark||',\n",
    "        '?' : '||question_mark||',\n",
    "        '(' : '||left_parentheses||',\n",
    "        ')' : '||right_parentheses||',\n",
    "        '-' : '||dash||',\n",
    "        '\\n': '||new_line||'\n",
    "    }\n",
    "        \n",
    "    return punctuation_to_token\n",
    "\n",
    "# Test\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the data and save it\n",
    "\n",
    "As a last step, I'm going to do the actual pre-processing. The code will translate punctuation marks nad create dictionaries. It will save the results in a file, so you can easily load it and just continue from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process training data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "This code allows you to continue analysis whenever you come back to the notebook. Just run this cell and it will load the pre-processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "# Load the pre-processed data\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  21388\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique words\n",
    "print(\"Number of unique words: \", len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after the pre-processing the number of unique words is more accurate. There are about 20,000 of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "It's time to build the pipeline that will feed data to the model.\n",
    "\n",
    "If possible, you want to use GPU during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU during training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU during training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing data\n",
    "\n",
    "The neural network needs input in form of tensors. To give data this format I'm going to use `TensorDataset`. Then I'm going to pass the dataset into `DataLoader` which will work as an iterator and handle shuffling and batching.\n",
    "\n",
    "Now you know that batching will be handled by the data loader but I still need to provide the feature and target variables. What form should they take? The generator's job will be to provide new lines of text on the basis of existing ones. So assuming that text is coded as integers representing words like this:\n",
    "\n",
    "```\n",
    "text = [ 2, 5, 3, 7, 1, 2, 6, 8, 9, 4, ... ]\n",
    "```\n",
    "it makes sense to define features and targets in the following way:\n",
    "```\n",
    "x1 = [ 2, 5, 3, 7 ]  y1 = [ 1 ]\n",
    "x2 = [ 5, 3, 7, 1 ]  y2 = [ 2 ]\n",
    "x3 = [ 3, 7, 1, 2 ]  y3 = [ 6 ]\n",
    "```\n",
    "As you can see, each feature consists of 4 consecutive words from the original text, while the target is the next, fifth word. This will allow the model to learn how to predict text from existing scripts. The length of a sequence, which in the example is equal to 4, is just a hyperparameter and can be modified.\n",
    "\n",
    "Let's write a function that provides variables this way and wraps them in a data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size, shuffle = False):\n",
    "    '''Provide batches of data using data loader.\n",
    "    \n",
    "    Args:\n",
    "        words(list): TV scripts represented as a list of words coded as integers\n",
    "        sequence_length(int): length of each word sequence (feature)\n",
    "        batch_size(int): number of features and targets in each batch\n",
    "    \n",
    "    Returns: data loader providing batches of (x, y).\n",
    "    '''\n",
    "    n = len(words)\n",
    "    s = sequence_length\n",
    "    \n",
    "    features = []\n",
    "    targets = []\n",
    "    \n",
    "    # Iterate over text, word by word\n",
    "    for i in range(n):\n",
    "        # As long as it is still possible to form a sequence\n",
    "        if i + s < n:\n",
    "            # Add each next sequence of words as a feature\n",
    "            features.append([words[j] for j in range(i, i + s)])\n",
    "            # And the following word as a target\n",
    "            targets.append(words[i + s])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Transform features and targets into tensors\n",
    "    dataset = TensorDataset(Tensor(features), Tensor(targets))\n",
    "    # Return a dataloader\n",
    "    dataloader = DataLoader(dataset = dataset, \n",
    "                            batch_size = batch_size, \n",
    "                            drop_last = True,\n",
    "                            shuffle = shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will re-create data from the example so you can compare the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  [2, 5, 3, 7, 1, 2, 6, 8, 9, 4] \n",
      "\n",
      "batch 1:\n",
      "x1:  [2. 5. 3. 7.] y1:  1\n",
      "x2:  [5. 3. 7. 1.] y2:  2\n",
      "x3:  [3. 7. 1. 2.] y3:  6\n",
      "\n",
      "\n",
      "batch 2:\n",
      "x5:  [7. 1. 2. 6.] y5:  8\n",
      "x6:  [1. 2. 6. 8.] y6:  9\n",
      "x7:  [2. 6. 8. 9.] y7:  4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters from the example\n",
    "fake_text = [ 2, 5, 3, 7, 1, 2, 6, 8, 9, 4 ]\n",
    "seq_length = 4\n",
    "batch_size = 3\n",
    "\n",
    "data_loader = batch_data(fake_text, seq_length, batch_size)\n",
    "\n",
    "# Print text for comparison\n",
    "print('text: ', fake_text, '\\n')\n",
    "# Print batches\n",
    "for batch_n, (x_batch, y_batch) in enumerate(data_loader):\n",
    "    print('batch ' + str(batch_n + 1) + ':')\n",
    "    for i in range(batch_size):\n",
    "        j = i + batch_n * seq_length + 1\n",
    "        print('x' + str(j) + ': ', x_batch[i].numpy(), \n",
    "              'y' + str(j) + ': ', y_batch[i].numpy().astype(np.int))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice that the output looks exactly as in the example. The order is also preserved in the second batch.\n",
    "\n",
    "Additionally, I created a test function. It creates data loader 3 times, each time on a random set of parameters. It prints *All tests passed*, meaning that words are in a correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test_data_loader(int_text):\n",
    "    # Test data loader 3 times with a random sequence length and a batch size\n",
    "    for i in range(3):\n",
    "        sequence_len = np.random.randint(1, 11)\n",
    "        batch_size = int(np.random.choice([8, 16, 32, 64]))\n",
    "        _data_loader_single_test(int_text, sequence_len, batch_size)\n",
    "    # Print a message if all tests are successful\n",
    "    print(\"All tests passed\")\n",
    "\n",
    "def _data_loader_single_test(int_text, sequence_len, batch_size):\n",
    "    # Create a data loader\n",
    "    data_loader = batch_data(int_text, sequence_len, batch_size)\n",
    "    # Helper function\n",
    "    tensor_to_int = lambda tensor: tensor.numpy().astype(np.int).tolist()\n",
    "\n",
    "    # Test the first batch\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        # Test batch size\n",
    "        assert(len(x_batch) == batch_size)\n",
    "        assert(len(y_batch) == batch_size)\n",
    "        \n",
    "        # Test sequence length\n",
    "        assert(len(x_batch[0]) == sequence_len)\n",
    "        \n",
    "        # Test words sequences\n",
    "        for i in range (0, batch_size):\n",
    "            a = tensor_to_int(x_batch[i]) # sequence of feature words\n",
    "            b = int_text[i : i + sequence_len] # corresponding sequence from text\n",
    "            assert(a == b)\n",
    "\n",
    "            a = tensor_to_int(y_batch[i]) # target word\n",
    "            b = int_text[i + sequence_len] # corresponding target from text\n",
    "            assert(a == b)\n",
    "        # Stop test after first batch\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_data_loader(int_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
