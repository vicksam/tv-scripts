{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Scripts Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project\n",
    "\n",
    "Imagine your job was to produce a TV script for the next episode of a long running TV show. There were so many episodes and it is hard for you to come up with something new. Good news is you can use deep learning to write the script for you. There are many scripts from previous episodes that a nerual network can learn from. And what's important, the audience likes what it is used to, so it sounds like a perfect solution.\n",
    "\n",
    "In this project I'm going to train a neural network using scripts from the 9 seasons of Seinfeld show. Based on recognized patterns, it will be able to generate a new text. And I will use it to create a one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "If you look at the data file, you will find out that it contains lines from scripts, appended episode by episode. I'm loading them all as a one huge string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "# Load in data\n",
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can access `text` letter by letter. Here I print first 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \\n\\njerry: (pointi'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that new lines are represented by `\\n`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `view_line_range` I print out the first ten lines of text. Empty lines also count as lines, so in result it printed out 5 quotes.\n",
    "\n",
    "You can also see some dataset stats. Rough estimate of unique words is overshot as it considers everything between spaces as a single word. So it will consider *back!* and *back* as two different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544240293684143\n",
      "\n",
      "The lines 0 to 10:\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
      "\n",
      "george: are you through? \n",
      "\n",
      "jerry: you do of course try on, when you buy? \n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Determine which lines range to print\n",
    "view_line_range = (0, 10)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "\n",
    "While working with text data, it is useful to code words into numbers. I am going to do it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup tables\n",
    "\n",
    "I will create two dictionaries, `vocab_to_int` that will map words to integers and `int_to_vocab` that will do the opposite. These lookup tables will make it easy to translate between words and their integer codings later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    '''Create lookup tables for vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        text(str): The text of tv scripts split into words\n",
    "    \n",
    "    Return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    '''\n",
    "    # Transform text into a large tuple of unique words\n",
    "    vocab = tuple(set(text))\n",
    "    \n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    vocab_to_int = { vocab : i for i, vocab in int_to_vocab.items() }\n",
    "\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "# Test\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize punctuation\n",
    "\n",
    "Later, I'm going to split `text` into words using whitespaces. However, punctuation can cause some problems like for example recognizning *bye* and *bye!* as two different words. You can solve it by replacing every punctuation mark with some word token, like \".\" with \"||period||\". You might ask, why to add separators \"|\"? It's so during analysis you can distinguish between real words and the tokens.\n",
    "\n",
    "Now I'm going to create a dictionary that can map punctuation marks into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    '''Generate a dict to turn punctuation into a token.\n",
    "    \n",
    "    Return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    '''\n",
    "    punctuation_to_token = {\n",
    "        '.' : '||period||',\n",
    "        ',' : '||comma||',\n",
    "        '\"' : '||quotation_mark||',\n",
    "        ';' : '||semicolon||',\n",
    "        '!' : '||exclamation_mark||',\n",
    "        '?' : '||question_mark||',\n",
    "        '(' : '||left_parentheses||',\n",
    "        ')' : '||right_parentheses||',\n",
    "        '-' : '||dash||',\n",
    "        '\\n': '||new_line||'\n",
    "    }\n",
    "        \n",
    "    return punctuation_to_token\n",
    "\n",
    "# Test\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the data and save it\n",
    "\n",
    "As a last step, I'm going to do the actual pre-processing. The code will translate punctuation marks nad create dictionaries. It will save the results in a file, so you can easily load it and just continue from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process training data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "This code allows you to continue analysis whenever you come back to the notebook. Just run this cell and it will load the pre-processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "# Load the pre-processed data\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  21388\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique words\n",
    "print(\"Number of unique words: \", len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after the pre-processing the number of unique words is more accurate. There are about 20,000 of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "It's time to build the pipeline that will feed data to the model.\n",
    "\n",
    "If possible, you want to use GPU during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU during training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU during training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing data\n",
    "\n",
    "The neural network needs input in form of tensors. To give data this format I'm going to use `TensorDataset`. Then I'm going to pass the dataset into `DataLoader` which will work as an iterator and handle shuffling and batching.\n",
    "\n",
    "Now you know that batching will be handled by the data loader but I still need to provide the feature and target variables. What form should they take? The generator's job will be to provide new lines of text on the basis of existing ones. So assuming that text is coded as integers representing words like this:\n",
    "\n",
    "```\n",
    "text = [ 2, 5, 3, 7, 1, 2, 6, 8, 9, 4, ... ]\n",
    "```\n",
    "it makes sense to define features and targets in the following way:\n",
    "```\n",
    "x1 = [ 2, 5, 3, 7 ]  y1 = [ 1 ]\n",
    "x2 = [ 5, 3, 7, 1 ]  y2 = [ 2 ]\n",
    "x3 = [ 3, 7, 1, 2 ]  y3 = [ 6 ]\n",
    "```\n",
    "As you can see, each feature consists of 4 consecutive words from the original text, while the target is the next, fifth word. This will allow the model to learn how to predict text from existing scripts. The length of a sequence, which in the example is equal to 4, is just a hyperparameter and can be modified.\n",
    "\n",
    "Let's write a function that provides variables this way and wraps them in a data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size, shuffle = False):\n",
    "    '''Provide batches of data using data loader.\n",
    "    \n",
    "    Args:\n",
    "        words(list): TV scripts represented as a list of words coded as integers\n",
    "        sequence_length(int): length of each word sequence (feature)\n",
    "        batch_size(int): number of features and targets in each batch\n",
    "    \n",
    "    Returns: data loader providing batches of (x, y).\n",
    "    '''\n",
    "    n = len(words)\n",
    "    s = sequence_length\n",
    "    \n",
    "    features = []\n",
    "    targets = []\n",
    "    \n",
    "    # Iterate over text, word by word\n",
    "    for i in range(n):\n",
    "        # As long as it is still possible to form a sequence\n",
    "        if i + s < n:\n",
    "            # Add each next sequence of words as a feature\n",
    "            features.append([words[j] for j in range(i, i + s)])\n",
    "            # And the following word as a target\n",
    "            targets.append(words[i + s])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Transform features and targets into tensors\n",
    "    dataset = TensorDataset(Tensor(features), Tensor(targets))\n",
    "    # Return a dataloader\n",
    "    dataloader = DataLoader(dataset = dataset, \n",
    "                            batch_size = batch_size, \n",
    "                            drop_last = True,\n",
    "                            shuffle = shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will re-create data from the example so you can compare the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  [2, 5, 3, 7, 1, 2, 6, 8, 9, 4] \n",
      "\n",
      "batch 1:\n",
      "x1:  [2. 5. 3. 7.] y1:  1\n",
      "x2:  [5. 3. 7. 1.] y2:  2\n",
      "x3:  [3. 7. 1. 2.] y3:  6\n",
      "\n",
      "\n",
      "batch 2:\n",
      "x5:  [7. 1. 2. 6.] y5:  8\n",
      "x6:  [1. 2. 6. 8.] y6:  9\n",
      "x7:  [2. 6. 8. 9.] y7:  4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters from the example\n",
    "fake_text = [ 2, 5, 3, 7, 1, 2, 6, 8, 9, 4 ]\n",
    "seq_length = 4\n",
    "batch_size = 3\n",
    "\n",
    "data_loader = batch_data(fake_text, seq_length, batch_size)\n",
    "\n",
    "# Print text for comparison\n",
    "print('text: ', fake_text, '\\n')\n",
    "# Print batches\n",
    "for batch_n, (x_batch, y_batch) in enumerate(data_loader):\n",
    "    print('batch ' + str(batch_n + 1) + ':')\n",
    "    for i in range(batch_size):\n",
    "        j = i + batch_n * seq_length + 1\n",
    "        print('x' + str(j) + ': ', x_batch[i].numpy(), \n",
    "              'y' + str(j) + ': ', y_batch[i].numpy().astype(np.int))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice that the output looks exactly as in the example. The order is also preserved in the second batch.\n",
    "\n",
    "Additionally, I created a test function. It creates data loader 3 times, each time on a random set of parameters. It prints *All tests passed*, meaning that words are in a correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test_data_loader(int_text):\n",
    "    # Test data loader 3 times with a random sequence length and a batch size\n",
    "    for i in range(3):\n",
    "        sequence_len = np.random.randint(1, 11)\n",
    "        batch_size = int(np.random.choice([8, 16, 32, 64]))\n",
    "        _data_loader_single_test(int_text, sequence_len, batch_size)\n",
    "    # Print a message if all tests are successful\n",
    "    print(\"All tests passed\")\n",
    "\n",
    "def _data_loader_single_test(int_text, sequence_len, batch_size):\n",
    "    # Create a data loader\n",
    "    data_loader = batch_data(int_text, sequence_len, batch_size)\n",
    "    # Helper function\n",
    "    tensor_to_int = lambda tensor: tensor.numpy().astype(np.int).tolist()\n",
    "\n",
    "    # Test the first batch\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        # Test batch size\n",
    "        assert(len(x_batch) == batch_size)\n",
    "        assert(len(y_batch) == batch_size)\n",
    "        \n",
    "        # Test sequence length\n",
    "        assert(len(x_batch[0]) == sequence_len)\n",
    "        \n",
    "        # Test words sequences\n",
    "        for i in range (0, batch_size):\n",
    "            a = tensor_to_int(x_batch[i]) # sequence of feature words\n",
    "            b = int_text[i : i + sequence_len] # corresponding sequence from text\n",
    "            assert(a == b)\n",
    "\n",
    "            a = tensor_to_int(y_batch[i]) # target word\n",
    "            b = int_text[i + sequence_len] # corresponding target from text\n",
    "            assert(a == b)\n",
    "        # Stop test after first batch\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_data_loader(int_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RNN\n",
    "\n",
    "I'm going to define the neural network architecture. I will use a bunch of parameters that will allow you for its customization.\n",
    "\n",
    "It will have 3 methods:\n",
    " - `__init__` - responsible for setting class variables\n",
    " - `init_hidden` - responsible for initializing hidden state\n",
    " - `forward` - responsible for flow of data during a forward pass\n",
    "\n",
    "Basically, what I want the network to do is:\n",
    "- take a batch of word sequences\n",
    "- code them as word embeddings\n",
    "- input them into LSTM layer\n",
    "- flatten its output\n",
    "- put it into fully-connected layer\n",
    "- deflatten the result\n",
    "- output a batch of predictions for every last-in-a-sequence word\n",
    "\n",
    "You may wonder why the output is only the last predicted word. Let's say that the input sequence are three words [B, F, C]. Then, they will be processed in parallel, by units LSTM-0, LSTM-1 and LSTM-2. Each of these units will try to predict the next word in a sequence, but you already know that after B comes F and that after F comes C. You can tell it from the original sequence. The only interesting prediction is that for the last word, C, because you can't guess it just by looking at the input sequence.\n",
    "\n",
    "And it is the idea behind the model that I mentioned before, to predict the next word given a sequence of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 output_size, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 n_layers, \n",
    "                 dropout = 0.5):\n",
    "        '''Initialize RNN's parameters and layers.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size(int): length of the input, one-hot encoded vector \n",
    "                             (equal to the vocab size)\n",
    "            output_size(int): number of output nodes\n",
    "            embedding_dim(int): length of embedding vector\n",
    "                                (how many digits will be used to represent a word)\n",
    "            hidden_dim(int): number of hidden nodes in the LSTM and FC layers\n",
    "            dropout(float): dropout applied to the output of LSTM layer\n",
    "        '''\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Set class variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        ## Define model layers\n",
    "        # Takes in a one hot-encoded vector (length equal to the number \n",
    "        # of unique words in a dict), outputs numerical code for that\n",
    "        # word like 1234\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size = self.embedding_dim,\n",
    "                            hidden_size = self.hidden_dim,\n",
    "                            num_layers = self.n_layers,\n",
    "                            dropout = self.dropout,\n",
    "                            batch_first = True)\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        '''Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            nn_input(tensor): input (batch_size, sequence_length)\n",
    "                              each observation is a list of ints\n",
    "                              representing words (vocab_to_int)\n",
    "            hidden(tuple): the hidden state, a tuple of tensors\n",
    "                           (n_layers, batch_size, hidden_dim)      \n",
    "        \n",
    "        Returns: tuple of output and hidden state\n",
    "        '''   \n",
    "        # Get the batch size (first input dimension)\n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        # Get embedding for the input, each word is encoded as a number\n",
    "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embed_input = self.embed(nn_input.long())\n",
    "        \n",
    "        # Get the outputs and the new hidden state from the LSTM:\n",
    "        # output (batch_size, sequence_length, hidden_dim)\n",
    "        # hidden state (n_layers, batch_size, hidden_dim)\n",
    "        lstm_output, hidden = self.lstm(embed_input, hidden)\n",
    "\n",
    "        # Flatten the output before feeding it into fc layer\n",
    "        # From 3D to 2D, (batch_size * sequence_length, hidden_dim)\n",
    "        # It works because fc layer can take 2D input like \n",
    "        # (batch_size, hidden_dim)\n",
    "        lstm_output = lstm_output.contiguous().view(-1, self.hidden_dim)\n",
    "        # View represents only a view of original object without\n",
    "        # actual change in memory. contiguous() causes an actual\n",
    "        # change in memory that matches a view\n",
    "        \n",
    "        # Feed into fc\n",
    "        # Returns a one-hot encoded vector for every input word\n",
    "        # (batch_size, hidden_dim) -> (batch_size, vocab_size)\n",
    "        output = self.fc(lstm_output)\n",
    "        \n",
    "        # Reshape the output to the desired output size\n",
    "        # Basically just return to the 3D representation\n",
    "        # starting with (batch_size, seq_length, ...)\n",
    "        # (batch_size, vocab_size) -> (batch_size, sequence_length, vocab_size)\n",
    "        output = output.view(batch_size, -1, self.output_size)\n",
    "\n",
    "        # Get the output (batch_size, vocab_size)\n",
    "        # Get prediction for the last word in the input sequence\n",
    "        out = output[:, -1]\n",
    "        # For every observation in a batch it is a vector of (vocab_size, )\n",
    "        # with probability  for each word (actually logit, because it is \n",
    "        # before applying softmax by the loss function)\n",
    "        \n",
    "        # Return one batch of predictions and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''Initialize the hidden state of an LSTM.\n",
    "        \n",
    "        Args:\n",
    "            batch_size(int)\n",
    "        \n",
    "        Returns: hidden state (n_layers, batch_size, hidden_dim)\n",
    "                 For every LSTM layer, it outputs a batch of hidden state\n",
    "                 output signals for each word.\n",
    "        '''\n",
    "        # Get weight matrix\n",
    "        weight = next(self.parameters()).data\n",
    "        # Create matrix of zero weights based on its shape\n",
    "        zero_weight = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "        \n",
    "        # Initialize hidden state with zero weights, and move to GPU if available\n",
    "        if train_on_gpu:\n",
    "            hidden = (zero_weight.cuda(), zero_weight.cuda())\n",
    "        else:\n",
    "            hidden = (zero_weight, zero_weight)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "# Test\n",
    "tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and backprop\n",
    "\n",
    "I am going to write a function that performs forward and back passes through network. I will call it later in the training loop like this:\n",
    "```\n",
    "loss = forward_back_prop(decoder, decoder_optimizer, criterion, inp, target)\n",
    "```\n",
    "It will make the code easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def forward_back_prop(rnn, \n",
    "                      optimizer, \n",
    "                      criterion, \n",
    "                      feature, \n",
    "                      target, \n",
    "                      hidden, \n",
    "                      clip_value = 1):\n",
    "    '''Forward and backward propagation run.\n",
    "    \n",
    "    Args:\n",
    "        rnn(RNN object): instance of the model object\n",
    "        optimizer(object): PyTorch optimizer\n",
    "        criterion(object): PyTorch loss function\n",
    "        feature(tensor): feature tensor (batch_size, sequence_length, vocab_size)\n",
    "        target(tensor): target tensor (batch_size, vocab_size)\n",
    "    \n",
    "    Returns: loss and the latest hidden state tensor\n",
    "    '''   \n",
    "    # Move data to GPU, if available\n",
    "    if train_on_gpu:\n",
    "        feature, target = feature.cuda(), target.cuda()\n",
    "    \n",
    "    # Detach hidden state from its history\n",
    "    # Otherwise it will backprop through entire training history\n",
    "    # and take a long time\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    output, hidden = rnn.forward(feature, hidden)\n",
    "    # Calculate the batch loss\n",
    "    loss = criterion(output, target.long())\n",
    "    # Backward pass\n",
    "    loss.backward(retain_graph = True)\n",
    "    # Clip gradient to prevent it from exploading\n",
    "    # If gradient > clip threshold, set its value to the clip threshold\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), clip_value)\n",
    "    # Parameter update\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return the loss over a batch and the hidden state produced by thw model\n",
    "    return loss.item(), hidden\n",
    "\n",
    "# Test\n",
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Architecture is ready, so it's the time for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop\n",
    "\n",
    "I'm going to write a code for a training loop using `forward_back_prop` that you saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches = 100):\n",
    "    # Initialize loss tracker\n",
    "    min_loss = np.Inf\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # Make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset) // batch_size\n",
    "            if batch_i > n_batches:\n",
    "                break\n",
    "            \n",
    "            # Forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # Record loss\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Print loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                # Calculate the loss in this epoch until now\n",
    "                loss = np.average(losses)\n",
    "                # Print\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {:.6f}\\n'.format(\n",
    "                    epoch_i, n_epochs, loss))\n",
    "        \n",
    "        # Calculate final epoch loss\n",
    "        loss = np.average(losses)\n",
    "        # Clear the loss record before new epoch\n",
    "        losses = []\n",
    "        \n",
    "        # Check if loss has improved\n",
    "        if loss < min_loss:\n",
    "            print('Epoch loss decreased ({:.6f} --> {:.6f}).  Saving model ...\\n'.format(\n",
    "                min_loss,\n",
    "                loss)\n",
    "            )\n",
    "            # Save the model\n",
    "            helper.save_model('model', rnn)\n",
    "            # Update min loss\n",
    "            min_loss = loss\n",
    "\n",
    "    # Return a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Now I will set the network hyperparameters. There is a bunch of them and you can always tune them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data params\n",
    "\n",
    "# Sequence Length\n",
    "sequence_length = 5  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "\n",
    "# Data loader - init with data params\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training params\n",
    "\n",
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "## Model parameters\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = len(vocab_to_int)\n",
    "# Embedding Dimension\n",
    "embedding_dim = 12\n",
    "# Hidden Dimension\n",
    "hidden_dim = 200\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 6969"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now you can actually run the training code. My goal is to reach the loss below 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epoch(s)...\n",
      "Epoch:    1/10    Loss: 4.866976\n",
      "\n",
      "Epoch loss decreased (inf --> 4.866976).  Saving model ...\n",
      "\n",
      "Epoch:    2/10    Loss: 4.234397\n",
      "\n",
      "Epoch loss decreased (4.866976 --> 4.234397).  Saving model ...\n",
      "\n",
      "Epoch:    3/10    Loss: 4.003606\n",
      "\n",
      "Epoch loss decreased (4.234397 --> 4.003606).  Saving model ...\n",
      "\n",
      "Epoch:    4/10    Loss: 3.853343\n",
      "\n",
      "Epoch loss decreased (4.003606 --> 3.853343).  Saving model ...\n",
      "\n",
      "Epoch:    5/10    Loss: 3.746055\n",
      "\n",
      "Epoch loss decreased (3.853343 --> 3.746055).  Saving model ...\n",
      "\n",
      "Epoch:    6/10    Loss: 3.666510\n",
      "\n",
      "Epoch loss decreased (3.746055 --> 3.666510).  Saving model ...\n",
      "\n",
      "Epoch:    7/10    Loss: 3.601086\n",
      "\n",
      "Epoch loss decreased (3.666510 --> 3.601086).  Saving model ...\n",
      "\n",
      "Epoch:    8/10    Loss: 3.553352\n",
      "\n",
      "Epoch loss decreased (3.601086 --> 3.553352).  Saving model ...\n",
      "\n",
      "Epoch:    9/10    Loss: 3.509620\n",
      "\n",
      "Epoch loss decreased (3.553352 --> 3.509620).  Saving model ...\n",
      "\n",
      "Epoch:   10/10    Loss: 3.473532\n",
      "\n",
      "Epoch loss decreased (3.509620 --> 3.473532).  Saving model ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout = 0.5)\n",
    "# Move it to GPU if available\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# Define loss and optimizer\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "My general idea during model tuning was to keep the model simple. If e.g. increase in hidden dimension improved the loss but only slightly, I would move back to the previous value.\n",
    "\n",
    "`seq_length` affects how the model works. Just think about how prediction based on 5 words can differ from a one based on 10. Text generated by the latter model is probably more consistent as it can get a better grasp of context, since there are more words to learn from. I kept this value low, at 5 as it should still perform fine.\n",
    "\n",
    "The general good value for `batch_size` is either 32, 64, 128 or 256. I decided to keep it at 128. As you may know, training accuracy is proportional to `batch_size` * `learning_rate` so I decided to focus on tuning the learning rate instead.\n",
    "\n",
    "As for `learning_rate`, I tried both 0.005 and 0.0005, but it seemed that 0.001 works better.\n",
    "\n",
    "You can remember that on the input and output of the network there is a one-hot encoded vector. `vocab_size` and `output_size` simply determine its length. And you should set it to the number of unique words.\n",
    "\n",
    "According to the Google Developers' blog entry, there is a simple formula for a good `embedding_dim` value. It is the 4th root of the number of classes. Here, classes refer to unique words, so the result is approximately 21,000 ** 0.25 ~ 12.\n",
    "\n",
    "I just set `num_epochs` to 10. It is a minimum value at which I was able to go below loss of 3.5.\n",
    "\n",
    "Generally, a good value of `n_layers` is between 1 to 3. The middle 2 was enough to get good results.\n",
    "\n",
    "As with most parameters, you need to experiment. I tried a range of parameters for `hidden_dim` and the loss decreases nicely at 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "\n",
    "if train_on_gpu:\n",
    "    map_location = lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location = 'cpu'\n",
    "\n",
    "trained_rnn = helper.load_model('model', map_location = map_location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
