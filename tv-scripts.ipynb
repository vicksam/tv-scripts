{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Scripts Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project\n",
    "\n",
    "Imagine your job was to produce a TV script for the next episode of a long running TV show. There were so many episodes and it is hard for you to come up with something new. Good news is you can use deep learning to write the script for you. There are many scripts from previous episodes that a nerual network can learn from. And what's important, the audience likes what it is used to, so it sounds like a perfect solution.\n",
    "\n",
    "In this project I'm going to train a neural network using scripts from the 9 seasons of Seinfeld show. Based on recognized patterns, it will be able to generate a new text. And I will use it to create a one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "If you look at the data file, you will find out that it contains lines from scripts, appended episode by episode. I'm loading them all as a one huge string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "# Load in data\n",
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can access `text` letter by letter. Here I print first 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \\n\\njerry: (pointi'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that new lines are represented by `\\n`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `view_line_range` I print out the first ten lines of text. Empty lines also count as lines, so in result it printed out 5 quotes.\n",
    "\n",
    "You can also see some dataset stats. Rough estimate of unique words is overshot as it considers everything between spaces as a single word. So it will consider *back!* and *back* as two different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544240293684143\n",
      "\n",
      "The lines 0 to 10:\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
      "\n",
      "george: are you through? \n",
      "\n",
      "jerry: you do of course try on, when you buy? \n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Determine which lines range to print\n",
    "view_line_range = (0, 10)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "\n",
    "While working with text data, it is useful to code words into numbers. I am going to do it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup tables\n",
    "\n",
    "I will create two dictionaries, `vocab_to_int` that will map words to integers and `int_to_vocab` that will do the opposite. These lookup tables will make it easy to translate between words and their integer codings later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    '''Create lookup tables for vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        text(str): The text of tv scripts split into words\n",
    "    \n",
    "    Return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    '''\n",
    "    # Transform text into a large tuple of unique words\n",
    "    vocab = tuple(set(text))\n",
    "    \n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    vocab_to_int = { vocab : i for i, vocab in int_to_vocab.items() }\n",
    "\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "# Test\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize punctuation\n",
    "\n",
    "Later, I'm going to split `text` into words using whitespaces. However, punctuation can cause some problems like for example recognizning *bye* and *bye!* as two different words. You can solve it by replacing every punctuation mark with some word token, like \".\" with \"||period||\". You might ask, why to add separators \"|\"? It's so during analysis you can distinguish between real words and the tokens.\n",
    "\n",
    "Now I'm going to create a dictionary that can map punctuation marks into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    '''Generate a dict to turn punctuation into a token.\n",
    "    \n",
    "    Return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    '''\n",
    "    punctuation_to_token = {\n",
    "        '.' : '||period||',\n",
    "        ',' : '||comma||',\n",
    "        '\"' : '||quotation_mark||',\n",
    "        ';' : '||semicolon||',\n",
    "        '!' : '||exclamation_mark||',\n",
    "        '?' : '||question_mark||',\n",
    "        '(' : '||left_parentheses||',\n",
    "        ')' : '||right_parentheses||',\n",
    "        '-' : '||dash||',\n",
    "        '\\n': '||new_line||'\n",
    "    }\n",
    "        \n",
    "    return punctuation_to_token\n",
    "\n",
    "# Test\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the data and save it\n",
    "\n",
    "As a last step, I'm going to do the actual pre-processing. The code will translate punctuation marks nad create dictionaries. It will save the results in a file, so you can easily load it and just continue from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process training data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "This code allows you to continue analysis whenever you come back to the notebook. Just run this cell and it will load the pre-processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "# Load the pre-processed data\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  21388\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique words\n",
    "print(\"Number of unique words: \", len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after the pre-processing the number of unique words is more accurate. There are about 20,000 of them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
